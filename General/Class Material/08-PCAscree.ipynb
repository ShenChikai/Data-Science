{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr/>\n",
    "\n",
    "# Introduction to Data Science\n",
    "**Tamás Budavári** - budavari@jhu.edu <br/>\n",
    "\n",
    "- Eigensystem of covariance matrix\n",
    "- Fitting lines and planes\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><font color=\"darkblue\">Recap from last time</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigendecomposition \n",
    "\n",
    "> $ C = E\\,\\Lambda\\,E^T$ and $E^T C\\,E = \\Lambda$ since $E^T E=I$\n",
    "><br/><br/>\n",
    "> or\n",
    "><br/><br/>\n",
    ">$\\displaystyle C = \\sum_{k=1}^N\\ \\lambda_k\\left(\\boldsymbol{e}_k\\,\\boldsymbol{e}_k^T\\right) \n",
    "= \\sum_{k=1}^N\\ \\lambda_k P_k $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scree Plot\n",
    "\n",
    "- The eigenvalue spectrum\n",
    "\n",
    ">$ \\big\\{ \\lambda_1, \\lambda_2, \\dots, \\lambda_N \\big\\}$\n",
    "\n",
    "- How many important directions?\n",
    "\n",
    "> Keep $K =\\,?$ principal components\n",
    "\n",
    "- Explained variance of components \n",
    "\n",
    "> Cf. $\\mathbb{Var}[X\\pm{}Y] = \\mathbb{Var}[X]+\\mathbb{Var}[Y]$ if uncorrelated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm as gaussian\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 10-D vectors: scale, rotate\n",
    "np.random.seed(1)\n",
    "Z = gaussian.rvs(0,1,(10,1000))\n",
    "if True: # scale them here\n",
    "    for i in range(Z[:,0].size): \n",
    "        Z[i,:] *= sqrt(i)\n",
    "    Z[:4,:] *= 1e-7\n",
    "    \n",
    "# quick-n-dirty random rotation\n",
    "M = random.randn(Z[:,0].size,Z[:,0].size)\n",
    "Q,_ = np.linalg.qr(M) # QR decomposition\n",
    "Y = Q @ Z # random rotation\n",
    "print (Y.shape)\n",
    "np.savetxt(\"temp.csv\", Y.T, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all previous variables from memory\n",
    "del Y, M, Q, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print (\"Shape of Y\" % Y.shape)\n",
    "except NameError as e:\n",
    "    print (\"Error message: %s\" % e)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze data\n",
    "\n",
    "Now your data file is available here: [temp.csv](temp.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas dataframe - table data structure\n",
    "df = pd.read_csv('temp.csv',header=None)\n",
    "df[:3] # slice like arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-load the Y matrix and convert dataframe to matrix\n",
    "Y = pd.read_csv('temp.csv',header=None).values.T\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot(111,aspect='equal')\n",
    "plot(Y[0,:], Y[6,:], '.', alpha=0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(n_components=Y.shape[0], whiten=False)\n",
    "B = pca.fit_transform(Y.T).T\n",
    "E, L = pca.components_.T, pca.explained_variance_\n",
    "\n",
    "subplot(1,1,1, aspect='equal')\n",
    "plot(B[0,:], B[5,:], '.', alpha=0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot(2,1,1); plot(L,'o-'); ylabel('Eigenvalues');\n",
    "subplot(2,1,2); cl=np.cumsum(L); ylabel('Total Variance');\n",
    "plot(cl/cl[-1],'o-r'); ylim(0,None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot orig and new covariance matrices (estimate w/o norm)\n",
    "ax=subplot(1,2,1); ax.matshow(Y @ Y.T); #Y.dot(Y.T) or np.dot(Y,Y.T)\n",
    "ax=subplot(1,2,2); ax.matshow(B @ B.T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = B[:6,:]\n",
    "# plot new covariance matrices with and without truncation\n",
    "ax=subplot(121); ax.matshow(B @ B.T);\n",
    "ax=subplot(122); ax.matshow(A @ A.T);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse of the Covariance Matrix\n",
    "\n",
    "- Appears in the multivariate normal distribution!\n",
    "\n",
    ">$\\displaystyle{\\cal{}N}(x;\\mu,C) = \\frac{1}{\\sqrt{\\det(2\\pi{}C)}}\\ \\exp\\left[-\\frac{1}{2}(x\\!-\\!\\mu)^T\\,C^{-1} (x\\!-\\!\\mu)\\right]$\n",
    "\n",
    "- Inverse of the diagonal eigenvalue matrix\n",
    "\n",
    ">$\\displaystyle \\Lambda^{-1} =  \\left( \\begin{array}{ccc}\n",
    "\\frac{1}{\\lambda_1} &  & \\cdots & 0\\\\\n",
    " & \\frac{1}{\\lambda_2} &   & \\vdots\\\\\n",
    "\\vdots &  & \\ddots &  \\\\\n",
    "0 & \\cdots &  & \\frac{1}{\\lambda_N} \\\\\n",
    "\\end{array} \\right)$\n",
    "\n",
    "- Inverse of the covariance matrix\n",
    "\n",
    ">$\\displaystyle C^{-1} = E\\ \\Lambda^{-1} E^T$\n",
    "\n",
    "- Also see pseudoinverse with small eigenvalues "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Lines\n",
    "\n",
    "- What if $x$ and $y$ are both noisy? \n",
    "\n",
    "> For example, $\\big\\{(x_i,y_i)\\big\\}$ measurements have the same uncertainties. \n",
    "> The relevant residuals are perpendicular to the line.\n",
    "> Minimizing RMS of residuals is related to maximizing the sample variance along line!\n",
    "\n",
    "- Sounds like the PCA problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### Fitting Planes\n",
    "\n",
    "- Similarly, fitting a $K$-dimensional hyperplane in $N$ dimensions, i.e., looking for the $a$ normal vector to minimize residuals on set of centered $\\{x_i\\}$ vectors\n",
    "\n",
    "> Minimizing sum of square lengths of the residual vectors\n",
    "><br/><br/>\n",
    ">$\\displaystyle \\qquad \\min_a \\sum_i r_i^2(a) \\ \\ \\ \\ \\ $  where $\\ \\ \\ r_i(a) = x_i - \\left(a\\,a^T\\right)x_i$, \n",
    "><br/><br/>\n",
    "> yields \n",
    "><br/><br/>\n",
    ">$\\displaystyle \\qquad \\min_a \\ \\left[\\textrm{const} - \\sum_i a^T\\!\\!\\left(x_i x_i^T\\right) a\\right]$\n",
    "><br/>\n",
    "or\n",
    "><br/>\n",
    ">$\\displaystyle \\qquad \\max_a \\ a^T\\!\\left(\\sum_i x_i x_i^T\\right)\\, a $ \n",
    "><br/><br/>\n",
    "> cf. sample variance along $a$, if data already centered\n",
    "\n",
    "- Essentially same as the PCA problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 2D (column) vectors\n",
    "np.random.seed(seed=42);\n",
    "N = gaussian.rvs(0,1,(2,50)); \n",
    "N[0,:] *= 2 \n",
    "f = +pi/6   # rotate by 30 deg\n",
    "R = array([[cos(f), -sin(f)],\n",
    "           [sin(f),  cos(f)]]) \n",
    "X = R @ N\n",
    "\n",
    "figure(figsize=(5,5)); subplot(111,aspect='equal');\n",
    "plot(X[0,:],X[1,:],'x',alpha=0.6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# project on 1st pricipal component\n",
    "X -= X.mean(axis=1, keepdims=True) \n",
    "E,_,_ = linalg.svd(X) # only the eigenvectors\n",
    "F = E[:,:1]           # truncated basis: only PC1\n",
    "P = F @ F.T @ X       # projection\n",
    "R = X - P             # residuals\n",
    "\n",
    "figure(figsize=(5,5)); subplot(111,aspect='equal');\n",
    "plot(X[0,:],X[1,:],'xb',alpha=0.6);\n",
    "plot(P[0,:],P[1,:],'or',alpha=0.4);\n",
    "quiver(P[0,:],P[1,:],R[0,:],R[1,:], alpha=0.2,\n",
    "    angles='xy',scale_units='xy',scale=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on Fitting Later\n",
    "\n",
    "- Next: Bayesian inference"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
